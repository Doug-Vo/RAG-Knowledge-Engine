{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28ee978",
   "metadata": {},
   "source": [
    "**Demo Model** \n",
    "This is how I retrieved the data from PDF, link (and Youtube), split them into chunks, embedded them and upload them to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pymongo import MongoClient\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
    "\n",
    "# Define the database and collection names\n",
    "MONGO_URI = os.environ.get('MONGO_URI')\n",
    "DB_NAME = \"ai_workbench\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "ATLAS_VECTOR_SEARCH_IDX = \"default\" \n",
    "\n",
    "# I commented them out to prevent duplications on multiple run\n",
    "# Uncomment to pick which one you want to upload to your MongoDB server\n",
    "sources = [\n",
    "    # \"doc/Stack & Queue.pdf\",  # Example local PDF file\n",
    "    # \"https://en.wikipedia.org/wiki/Oulu\", # Example web page\n",
    "    # \"https://youtu.be/yhiauaA5bc8?si=te3ra0DrY-F-xjvU\" # Example YouTube video\n",
    "    # \"https://youtu.be/jGwO_UgTS7I?si=WBSTHpb4LJikOFJi\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1cd81",
   "metadata": {},
   "source": [
    "I faced some complications with extracting the video, mainly with faulty api and extensions, I did not want to give in and use OpenAI API.\n",
    "\n",
    "Finally I found pytubefix to be able to extract the captions from video, and in case the captions was not English, I would use googletrans to translate them. This whole problem requires 2 additional helper function to operate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5bfb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "# Helper function to clean the raw SRT caption format from the Youtube's captions\n",
    "def clean_srt_captions(srt_text):\n",
    "    \"\"\"\n",
    "    Parses an SRT string and returns only the text, without\n",
    "    index numbers, timestamps, or empty lines.\n",
    "    \"\"\"\n",
    "    lines = srt_text.splitlines()\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        # Skip empty lines, lines that are just a number (index),\n",
    "        # and lines with the SRT timestamp arrow '-->'\n",
    "        if line.strip() and not line.strip().isdigit() and '-->' not in line:\n",
    "            clean_lines.append(line.strip())\n",
    "    return \" \".join(clean_lines)\n",
    "\n",
    "def translate_to_english(text, src_lang = 'auto'):\n",
    "    \"\"\"\n",
    "    Translate the text to English using googletrans\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = translator.translate(text, dest = 'en', src = src_lang)\n",
    "        return result.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bea6274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading https://youtu.be/jGwO_UgTS7I?si=WBSTHpb4LJikOFJi\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.docstore.document import Document\n",
    "from pytubefix import YouTube\n",
    "\n",
    "\n",
    "all_doc = []\n",
    "\n",
    "for source in sources:\n",
    "    print(f\"Reading {source}\")\n",
    "    try:\n",
    "        if(source.endswith('pdf')):         #Grabbing pdf\n",
    "            loader = PyPDFLoader(source)\n",
    "            all_doc.extend(loader.load())\n",
    "\n",
    "        elif 'youtube.com' in source or 'youtu.be' in source:       # Grabbing youtube\n",
    "            yt = YouTube(source)\n",
    "            caption = None\n",
    "\n",
    "            # Look up available captions and pick the best direction from there\n",
    "            if 'en' in yt.captions:\n",
    "                caption = yt.captions['en']\n",
    "\n",
    "            elif 'a.en' in yt.captions:\n",
    "                caption = yt.captions['a.en']\n",
    "\n",
    "            elif len(yt.captions > 0):\n",
    "                caption = list(yt.caption)[0]\n",
    "                print(\" No English caption available, imporvising..\")\n",
    "            \n",
    "            if caption:\n",
    "                processed_caption =  clean_srt_captions(caption.generate_srt_captions())\n",
    "                final_text = processed_caption\n",
    "                if caption.code != 'en' and caption.code != 'a.en':\n",
    "                    final_text = translate_to_english(processed_caption, src_lang= caption.code)\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content= final_text,\n",
    "                    metadata={\"source\": source, \"title\": yt.title, \"original_language\": caption.code}\n",
    "                )\n",
    "                all_doc.append(doc)\n",
    "            else:\n",
    "                raise ValueError(\"Could not found any available captions\")\n",
    "\n",
    "        elif source.startswith('https://'): #Grabbing links (notice we don't grab unsecured links)\n",
    "            loader = WebBaseLoader(source)\n",
    "            all_doc.extend(loader.load())\n",
    "        else:\n",
    "            print(f\"Unknown source file: {source}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\t\\t ERROR LOADING {source}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5340b",
   "metadata": {},
   "source": [
    "**Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65eeaa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Splitting...\n",
      " Splitted into 83 chunk(s)\n"
     ]
    }
   ],
   "source": [
    "print(\".. Splitting...\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(all_doc)\n",
    "print(f\" Splitted into {len(docs)} chunk(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac083a4",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8299d714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Initializing Embedding Model..\n"
     ]
    }
   ],
   "source": [
    "print(\"..Initializing Embedding Model..\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded3d20",
   "metadata": {},
   "source": [
    "**Upload to MongoDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82e1395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Connecting to MongoDB server\n"
     ]
    }
   ],
   "source": [
    "print(\".. Connecting to MongoDB server\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d6722c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Embedding to MongoDB..\n",
      ".... COMPLETE....\n"
     ]
    }
   ],
   "source": [
    "print(\"... Embedding to MongoDB..\")\n",
    "\n",
    "MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = embedding_model,\n",
    "    collection = collection,\n",
    "    index_name = ATLAS_VECTOR_SEARCH_IDX,\n",
    ")\n",
    "print(\".... COMPLETE....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6207dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
