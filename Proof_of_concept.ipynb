{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6ae5d6",
   "metadata": {},
   "source": [
    "# Production RAG Pipeline Evaluation (Proof of Concept)\n",
    "\n",
    "This notebook evaluates the production Retrieval-Augmented Generation (RAG) pipeline for the `AI Document Workbench` using the [**RAGAS**](https://docs.ragas.io/en/stable/) (Retrieval Augmented Generation Assessment) framework. \n",
    "\n",
    "Instead of testing a local sandbox, this Proof of Concept connects directly to the **MongoDB Atlas Vector Search** production database to evaluate real-world retrieval performance.\n",
    "\n",
    "**Key Objectives:**\n",
    "1. Fetch live document chunks from the production database.\n",
    "2. Generate synthetic test questions using `gpt-4o-mini`.\n",
    "3. Execute the questions through the production RAG chain.\n",
    "4. Grade the pipeline automatically using `gpt-4o` as an impartial judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ragas pandas ipykernel\n",
    "#!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74012ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from datasets import Dataset\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# RAGAS Imports (for evaluation)\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextPrecision\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import embedding_factory\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5acffd",
   "metadata": {},
   "source": [
    "## 1. Fetching Production Data\n",
    "We query the live **MongoDB Atlas** cluster and pull a random sample of document chunks that have already been embedded and indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268ef4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 5 random chunks from existing database...\n",
      "Fetched 5 chunks for testing.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Mongodb\n",
    "client = MongoClient(os.environ[\"MONGO_URI\"])\n",
    "DB_NAME = \"ai_workbench\"\n",
    "COLLECTION_NAME = \"documents\" \n",
    "collection = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "# Fetchching random chunks for testing\n",
    "print(\"Fetching 5 random chunks from existing database...\")\n",
    "\n",
    "pipeline = [{ \"$sample\": { \"size\": 5 } }]\n",
    "random_docs = list(collection.aggregate(pipeline))\n",
    "test_contexts = []\n",
    "for doc in random_docs:\n",
    "    text = doc.get(\"text\") or doc.get(\"page_content\") or \"\"\n",
    "    if text:\n",
    "        test_contexts.append(text)\n",
    "\n",
    "print(f\"Fetched {len(test_contexts)} chunks for testing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e72e8",
   "metadata": {},
   "source": [
    "## 2. Synthetic Test Generation (Manual Approach)\n",
    "We use `gpt-4o-mini` to act as an examiner. For each retrieved document chunk, the AI generates a specific, context-bound question and its corresponding \"Ground Truth\" answer. \n",
    "\n",
    "*Note: This simulatate our application with the same prompt, pipeline and dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd91f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions from our DB data...\n",
      " + Completed Q1\n",
      " + Completed Q2\n",
      " + Completed Q3\n",
      " + Completed Q4\n",
      " + Completed Q5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What significant legal decision does Trump cla...</td>\n",
       "      <td>Roe v. Wade</td>\n",
       "      <td>^ \"Tracking how many key positions Trump has f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is one advantage of linear regression men...</td>\n",
       "      <td>It has no local optimum.</td>\n",
       "      <td>as you know it's going down over time and then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What algorithm will be used to find a value of...</td>\n",
       "      <td>Gradient descent</td>\n",
       "      <td>we talk about when we talk about a generalizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who are included in the context provided?</td>\n",
       "      <td>All presidential candidates, Presidents, Third...</td>\n",
       "      <td>All presidential candidates\\n Presidents\\n Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What act is associated with the phrase \"No tax...</td>\n",
       "      <td>One Big Beautiful Bill Act</td>\n",
       "      <td>White House Faith Office\\nEconomic\\nArtificial...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What significant legal decision does Trump cla...   \n",
       "1  What is one advantage of linear regression men...   \n",
       "2  What algorithm will be used to find a value of...   \n",
       "3          Who are included in the context provided?   \n",
       "4  What act is associated with the phrase \"No tax...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                                        Roe v. Wade   \n",
       "1                           It has no local optimum.   \n",
       "2                                   Gradient descent   \n",
       "3  All presidential candidates, Presidents, Third...   \n",
       "4                         One Big Beautiful Bill Act   \n",
       "\n",
       "                                             context  \n",
       "0  ^ \"Tracking how many key positions Trump has f...  \n",
       "1  as you know it's going down over time and then...  \n",
       "2  we talk about when we talk about a generalizat...  \n",
       "3  All presidential candidates\\n Presidents\\n Thi...  \n",
       "4  White House Faith Office\\nEconomic\\nArtificial...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup Generator\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Exam Creator Prompt\n",
    "generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a professor creating an exam. \n",
    "Given the following text context, write ONE question that can be answered using ONLY this information.\n",
    "Also provide the correct answer (ground truth).\n",
    "\n",
    "Format your output exactly like this:\n",
    "QUESTION: [The question]\n",
    "ANSWER: [The correct answer]\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "chain = generation_prompt | generator_llm | StrOutputParser()\n",
    "\n",
    "# Generate\n",
    "data = []\n",
    "print(\"Generating questions from our DB data...\")\n",
    "\n",
    "for i, context in enumerate(test_contexts):\n",
    "    try:\n",
    "        output = chain.invoke({\"context\": context})\n",
    "        \n",
    "        # Simple parsing\n",
    "        parts = output.split(\"ANSWER:\")\n",
    "        question = parts[0].replace(\"QUESTION:\", \"\").strip()\n",
    "        ground_truth = parts[1].strip() if len(parts) > 1 else \"Error parsing\"\n",
    "        \n",
    "        data.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"context\": context # Keep context for reference\n",
    "        })\n",
    "        print(f\" + Completed Q{i+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" - Failed Q{i+1}: {e}\")\n",
    "\n",
    "# Save to DataFrame\n",
    "test_df = pd.DataFrame(data)\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a598618",
   "metadata": {},
   "source": [
    "## 3. RAG Pipeline Execution\n",
    "We pass the synthetic questions into the exact LangChain retrieval and generation pipeline used in the live `app.py`. We record the generated `answer` and the `retrieved_contexts` for the final grading step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136b1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering questions using Production RAG...\n",
      "Answers generated. Sample results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What significant legal decision does Trump cla...</td>\n",
       "      <td>Roe v. Wade</td>\n",
       "      <td>^ \"Tracking how many key positions Trump has f...</td>\n",
       "      <td>Trump claims to have influenced the overturnin...</td>\n",
       "      <td>[Multiple analyses conducted by academic schol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is one advantage of linear regression men...</td>\n",
       "      <td>It has no local optimum.</td>\n",
       "      <td>as you know it's going down over time and then...</td>\n",
       "      <td>One advantage of linear regression mentioned i...</td>\n",
       "      <td>[as you know it's going down over time and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What algorithm will be used to find a value of...</td>\n",
       "      <td>Gradient descent</td>\n",
       "      <td>we talk about when we talk about a generalizat...</td>\n",
       "      <td>The algorithm that will be used to find a valu...</td>\n",
       "      <td>[the normal equation looks like so arms of thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who are included in the context provided?</td>\n",
       "      <td>All presidential candidates, Presidents, Third...</td>\n",
       "      <td>All presidential candidates\\n Presidents\\n Thi...</td>\n",
       "      <td>The context provided includes the following in...</td>\n",
       "      <td>[Category\\n List, All presidential candidates\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What act is associated with the phrase \"No tax...</td>\n",
       "      <td>One Big Beautiful Bill Act</td>\n",
       "      <td>White House Faith Office\\nEconomic\\nArtificial...</td>\n",
       "      <td>The phrase \"No tax on tips\" is associated with...</td>\n",
       "      <td>[One Big Beautiful Bill Act, In July 2025, Tru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What significant legal decision does Trump cla...   \n",
       "1  What is one advantage of linear regression men...   \n",
       "2  What algorithm will be used to find a value of...   \n",
       "3          Who are included in the context provided?   \n",
       "4  What act is associated with the phrase \"No tax...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                                        Roe v. Wade   \n",
       "1                           It has no local optimum.   \n",
       "2                                   Gradient descent   \n",
       "3  All presidential candidates, Presidents, Third...   \n",
       "4                         One Big Beautiful Bill Act   \n",
       "\n",
       "                                             context  \\\n",
       "0  ^ \"Tracking how many key positions Trump has f...   \n",
       "1  as you know it's going down over time and then...   \n",
       "2  we talk about when we talk about a generalizat...   \n",
       "3  All presidential candidates\\n Presidents\\n Thi...   \n",
       "4  White House Faith Office\\nEconomic\\nArtificial...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Trump claims to have influenced the overturnin...   \n",
       "1  One advantage of linear regression mentioned i...   \n",
       "2  The algorithm that will be used to find a valu...   \n",
       "3  The context provided includes the following in...   \n",
       "4  The phrase \"No tax on tips\" is associated with...   \n",
       "\n",
       "                                  retrieved_contexts  \n",
       "0  [Multiple analyses conducted by academic schol...  \n",
       "1  [as you know it's going down over time and the...  \n",
       "2  [the normal equation looks like so arms of thi...  \n",
       "3  [Category\\n List, All presidential candidates\\...  \n",
       "4  [One Big Beautiful Bill Act, In July 2025, Tru...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup Retriever (Same as app.py)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=collection,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"default\" \n",
    ")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Define Chain (Same as app.py)\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert research assistant. Your goal is to provide accurate, well-structured answers based STRICTLY on the provided context.\n",
    "\n",
    "    Instructions:\n",
    "    1. Use ONLY the context provided below. DO NOT use outside knowledge.\n",
    "    2. Write in PLAIN TEXT only. Do NOT use Markdown formatting.\n",
    "    3. STRICTLY FORBIDDEN: Do not use bold (**text**), italics (*text*), or headers (#).\n",
    "    4. You may use simple hyphens (-) for lists, but no other styling.\n",
    "\n",
    "    Context:\n",
    "    {context}\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"input\": RunnablePassthrough()} \n",
    "    | rag_prompt\n",
    "    | generator_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run Evaluation Loop\n",
    "print(\"Answering questions using Production RAG...\")\n",
    "\n",
    "answers = []\n",
    "retrieved_contexts = []\n",
    "\n",
    "for q in test_df[\"question\"]:\n",
    "    # Get Answer\n",
    "    ans = rag_chain.invoke(q)\n",
    "    answers.append(ans)\n",
    "    \n",
    "    docs = retriever.invoke(q)\n",
    "    retrieved_contexts.append([d.page_content for d in docs])\n",
    "\n",
    "# Update Data\n",
    "test_df[\"answer\"] = answers\n",
    "test_df[\"retrieved_contexts\"] = retrieved_contexts\n",
    "\n",
    "\n",
    "print(\"Answers generated. Sample results:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302e87e",
   "metadata": {},
   "source": [
    "## 4. RAGAS Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "\n",
    "We use `gpt-4o` to grade the RAG pipeline across three critical metrics:\n",
    "\n",
    "* **Faithfulness (0.0 to 1.0):** Measures hallucination rate. A score of 1.0 means every claim in the generated answer is directly supported by the retrieved context.\n",
    "* **Context Precision (0.0 to 1.0):** Measures retrieval quality. A high score indicates that MongoDB Atlas successfully ranked the most relevant document chunks at the very top of the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c401f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading with RAGAS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddbaace28b54a49b2a99832262600ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scores:\n",
      "{'faithfulness': 0.8833, 'context_precision': 0.7111}\n"
     ]
    }
   ],
   "source": [
    "print(\"Grading with RAGAS...\")\n",
    "\n",
    "# Prepare Dataset\n",
    "eval_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Init OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Setup models\n",
    "wrapped_critic = llm_factory('gpt-4o', client=openai_client)\n",
    "wrapped_embeddings = embedding_factory('openai', model='text-embedding-3-small', client=openai_client)\n",
    "\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[\n",
    "        Faithfulness(),\n",
    "        ContextPrecision()\n",
    "    ],\n",
    "    llm=wrapped_critic,\n",
    "    embeddings=wrapped_embeddings\n",
    ")\n",
    "\n",
    "\n",
    "result.to_pandas().to_csv(\"rag_evaluation_final.csv\", index=False)\n",
    "print(\"Final Scores:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c27c6",
   "metadata": {},
   "source": [
    "### Final RAGAS Evaluation Results\n",
    "\n",
    "| user_input | retrieved_contexts | response | reference | faithfulness | context_precision |\n",
    "|:---|:---|:---|:---|---:|---:|\n",
    "| What significant legal decision does Trump claim to have influenced? | ['Multiple analyses conducted by academic scholars... | Trump claims to have influenced the overturning of... | Roe v. Wade... | 1.0000 | 0.5833 |\n",
    "| What is one advantage of linear regression mentioned in the text? | [\"as you know it's going down over time and then i... | One advantage of linear regression mentioned in th... | It has no local optimum.... | 1.0000 | 0.8333 |\n",
    "| What algorithm will be used to find a value of theta that minimizes the cost function J of theta? | [\"the normal equation looks like so arms of this d... | The algorithm that will be used to find a value of... | Gradient descent... | 1.0000 | 0.6389 |\n",
    "| Who are included in the context provided? | ['Category\\n List', 'All presidential candidates\\n... | The context provided includes the following indivi... | All presidential candidates, Presidents, Third-par... | 0.9167 | 0.5000 |\n",
    "| What act is associated with the phrase \"No tax on tips\"? | ['One Big Beautiful Bill Act', \"In July 2025, Trum... | The phrase \"No tax on tips\" is associated with the... | One Big Beautiful Bill Act... | 0.5000 | 1.0000 |\n",
    "\n",
    "\n",
    "\n",
    "### Analysis & Key Takeaways\n",
    "\n",
    "**1. Generator Performance (Faithfulness): `Excellent`**\n",
    "* The system scored 1.0 on 3 out of 5 questions.\n",
    "* **Meaning:** Our LLM (`gpt-4o-mini`) is highly reliable. It successfully restricts itself strictly to the provided context and avoids hallucinating outside knowledge. \n",
    "* **The Outlier:** Question 5 scored a 0.5. The LLM likely included extra conversational filler or the original material did not provide much data to begin with\n",
    "\n",
    "**2. Retriever Performance (Context Precision): Needs Tuning**\n",
    "* Scores fluctuated heavily (0.50 to 1.00). \n",
    "* **Meaning:** MongoDB Atlas Vector Search is finding the right information, but it is not always ranking the *best* chunk at the absolute top (Position #1). A score of 0.50 means the correct answer was likely the 2nd or 3rd chunk retrieved.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
